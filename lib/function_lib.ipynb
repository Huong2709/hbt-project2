{"cells":[{"cell_type":"code","execution_count":null,"id":"5b5129df","metadata":{"id":"5b5129df"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from underthesea import word_tokenize, pos_tag, sent_tokenize\n","import regex\n","import demoji\n","import emoji\n","from pyvi import ViPosTagger, ViTokenizer\n","import string\n","import re\n","import pickle"]},{"cell_type":"code","execution_count":null,"id":"68e0d22c","metadata":{"id":"68e0d22c"},"outputs":[],"source":["# ##LOAD EMOJICON\n","# file = open('files/emojicon.txt', 'r', encoding=\"utf8\")\n","# emoji_lst = file.read().split('\\n')\n","# emoji_dict = {}\n","# for line in emoji_lst:\n","#     key, value = line.split('\\t')\n","#     emoji_dict[key] = str(value)\n","# file.close()\n","# #################\n","# #LOAD TEENCODE\n","# file = open('files/teencode.txt', 'r', encoding=\"utf8\")\n","# teen_lst = file.read().split('\\n')\n","# teen_dict = {}\n","# for line in teen_lst:\n","#     key, value = line.split('\\t')\n","#     teen_dict[key] = str(value)\n","# file.close()\n","# ###############\n","# #LOAD TRANSLATE ENGLISH -> VNMESE\n","# file = open('files/english-vnmese.txt', 'r', encoding=\"utf8\")\n","# english_lst = file.read().split('\\n')\n","# english_dict = {}\n","# for line in english_lst:\n","#     key, value = line.split('\\t')\n","#     english_dict[key] = str(value)\n","# file.close()\n","# ################\n","# #LOAD wrong words\n","# file = open('files/wrong-word.txt', 'r', encoding=\"utf8\")\n","# wrong_lst = file.read().split('\\n')\n","# file.close()\n","# #################\n","# #LOAD STOPWORDS\n","# file = open('files/vietnamese-stopwords.txt', 'r', encoding=\"utf8\")\n","# stopwords_lst = file.read().split('\\n')\n","# file.close()"]},{"cell_type":"code","execution_count":null,"id":"4d8ca62a","metadata":{"id":"4d8ca62a"},"outputs":[],"source":["# def process_text(text, emoji_dict, teen_dict, wrong_lst):\n","#     document = text.lower()\n","#     document = document.replace(\"’\",'')\n","#     document = regex.sub(r'\\.+', \".\", document)\n","#     new_sentence =''\n","#     for sentence in sent_tokenize(document):\n","#         # if not(sentence.isascii()):\n","#         ###### CONVERT EMOJICON\n","#         sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))\n","#         ###### CONVERT TEENCODE\n","#         sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())\n","#         ###### DEL Punctuation & Numbers\n","#         pattern = r'(?i)\\b[a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]+\\b'\n","#         sentence = ' '.join(regex.findall(pattern,sentence))\n","#         ###### DEL wrong words   \n","#         sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())\n","#         new_sentence = new_sentence+ sentence + '. '                    \n","#     document = new_sentence  \n","#     #print(document)\n","#     ###### DEL excess blank space\n","#     document = regex.sub(r'\\s+', ' ', document).strip()\n","#     return document"]},{"cell_type":"code","execution_count":null,"id":"9116854e","metadata":{"id":"9116854e"},"outputs":[],"source":["# def process_text(text, emoji_dict, teen_dict, wrong_lst):\n","#     document = text.lower()\n","#     document = document.replace(\"’\",'')\n","#     document = regex.sub(r'\\.+', \".\", document)\n","#     document = regex.sub(r'\\s+', ' ', document).strip()\n","#     return document"]},{"cell_type":"code","execution_count":null,"id":"f807716e","metadata":{"id":"f807716e"},"outputs":[],"source":["# # Chuẩn hóa unicode tiếng việt\n","# def loaddicchar():\n","#     uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n","#     unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n","\n","#     dic = {}\n","#     char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n","#         '|')\n","#     charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n","#         '|')\n","#     for i in range(len(char1252)):\n","#         dic[char1252[i]] = charutf8[i]\n","#     return dic\n"," \n","# # Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n","# def convert_unicode(txt):\n","#     dicchar = loaddicchar()\n","#     return regex.sub(\n","#         r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n","#         lambda x: dicchar[x.group()], txt)"]},{"cell_type":"code","execution_count":null,"id":"3d7019d0","metadata":{"id":"3d7019d0"},"outputs":[],"source":["# # có thể bổ sung thêm các từ: chẳng, chả...\n","# def process_special_word(text):\n","#     new_text = ''\n","#     text_lst = text.split()\n","#     i= 0\n","#     if 'không' in text_lst:\n","#         while i <= len(text_lst) - 1:\n","#             word = text_lst[i]\n","#             #print(word)\n","#             #print(i)\n","#             if  word == 'không':\n","#                 next_idx = i+1\n","#                 if next_idx <= len(text_lst) -1:\n","#                     word = word +'_'+ text_lst[next_idx]\n","#                 i= next_idx + 1\n","#             else:\n","#                 i = i+1\n","#             new_text = new_text + word + ' '\n","#     else:\n","#         new_text = text\n","#     return new_text.strip()"]},{"cell_type":"code","execution_count":null,"id":"858edee3","metadata":{"id":"858edee3"},"outputs":[],"source":["# def process_postag_thesea(text):\n","#     new_document = ''\n","#     for sentence in sent_tokenize(text):\n","#         sentence = sentence.replace('.','')\n","#         ###### POS tag: A: Adjective, AB: Abbreviation, V:verb, VB:verb, VY:verb, R:Adverb\n","#         # lst_word_type = ['A','AB','V','VB','VY','R']\n","#         lst_word_type = ['A','AB','V','VB','VY','R']\n","#         sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format=\"text\"))))\n","#         new_document = new_document + sentence + ' '\n","#     ###### DEL excess blank space\n","#     new_document = regex.sub(r'\\s+', ' ', new_document).strip()\n","#     return new_document\n","\n","# # Trong bối cảnh bài toán sentiment thì thì dùng list như trên\n","# # Trong bối cảnh bài toán khác, cần POS tag khác thì thay đổi cho phù hợp\n","# # Search Google để biết các POS tag khác"]},{"cell_type":"code","execution_count":null,"id":"b1b7ff96","metadata":{"id":"b1b7ff96"},"outputs":[],"source":["# def remove_stopword(text, stopwords):\n","#     ###### REMOVE stop words\n","#     document = ' '.join('' if word in stopwords else word for word in text.split())\n","#     #print(document)\n","#     ###### DEL excess blank space\n","#     document = regex.sub(r'\\s+', ' ', document).strip()\n","#     return document"]},{"cell_type":"code","execution_count":null,"id":"0e785833","metadata":{"id":"0e785833"},"outputs":[],"source":["# def no_accent_vietnamese(s):\n","#     import re\n","#     s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n","#     s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n","#     s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n","#     s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n","#     s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n","#     s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n","#     s = re.sub('[íìỉĩị]', 'i', s)\n","#     s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n","#     s = re.sub('[úùủũụưứừửữự]', 'u', s)\n","#     s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n","#     s = re.sub('[ýỳỷỹỵ]', 'y', s)\n","#     s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n","#     s = re.sub('đ', 'd', s)\n","#     s = re.sub('Đ', 'D', s)\n","#     return s"]},{"cell_type":"markdown","id":"2iN3b17u2C3T","metadata":{"id":"2iN3b17u2C3T"},"source":["# Pipeline"]},{"cell_type":"markdown","id":"5xSjeykx1Q8n","metadata":{"id":"5xSjeykx1Q8n"},"source":["### Create def"]},{"cell_type":"code","execution_count":null,"id":"tWq9YRLst2N4","metadata":{"id":"tWq9YRLst2N4"},"outputs":[],"source":["def data_cleaning(data):\n","    df = data.copy()\n","\n","    # remove unnamed columns\n","    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n","\n","    # feature \"review_text\"\n","    df_clean = df\n","    document = df_clean['review_text']\n","\n","    # load edited emojicon\n","    file = open('lib/files/emojicon.txt', 'r', encoding=\"utf8\")\n","    emoji_lst = file.read().split('\\n')\n","    file.close()\n","\n","    emoji_dict = {}\n","    lst_key = []\n","    lst_value = []\n","    for line in emoji_lst:\n","      key, value = line.split('\\t')\n","      lst_key.append(key)\n","      lst_value.append(value)\n","      emoji_dict[key] = str(value)\n","\n","    # load chosen words\n","    file = open('lib/files/chosen_words_full.txt', 'r', encoding=\"utf8\")\n","    lst_chosen_words = file.read().split('\\n')\n","    file.close()\n","\n","    lst_spec_symbol = ['~','`','!','@','#','$','%','^','&','*','(',')','-','=','+',   '[','{','}','}','\\\\','|',';',':','\"',     ',','<','.','>','/','?']\n","    lst_find_words = ['không ','ko ','kg ','chẳng ','chả ']\n","    lst_replace_words = ['không_','ko_','kg_','chẳng_','chả_']\n","\n","    lst_document = []\n","    for doc in document:\n","      doc = doc.lower()\n","\n","    # TEXT CLEANING\n","    # replace emojicons\n","      for i in range(len(lst_key)):\n","        doc = doc.replace(lst_key[i], ' '+lst_value[i])\n","      for j in doc:\n","        if j in emoji.UNICODE_EMOJI['en']: \n","          doc = doc.replace(j, '')\n","\n","    # word tokenize: sạch sẽ => sạch_sẽ\n","      doc = word_tokenize(doc, format='text')\n","\n","    # remove special symbols\n","      rx = '[' + re.escape(''.join(lst_spec_symbol)) + ']' \n","      doc = re.sub(rx, '', doc)\n","\n","      doc = doc.replace('  ',' ')\n","      doc = doc.replace(' _',' ')\n","      doc = doc.replace('_ ',' ')\n","\n","    # replace 'không ' to 'không_' co link words, v.v...\n","      for i in range(len(lst_find_words)):\n","        doc = doc.replace(lst_find_words[i], lst_replace_words[i]) \n","\n","    # remove stop_words\n","      lst_words = []\n","      for j in doc.split(' '):\n","        if j in lst_chosen_words: lst_words.append(j)\n","      doc = ' '.join(lst_words)  \n","\n","      lst_document.append(doc)\n","\n","    df_clean['review_text_clean'] = lst_document\n","    # df_clean = df_clean.dropna()\n","\n","    # create \"review_score_new\"\n","    df_clean['review_score_new'] = 1\n","    df_clean.loc[df_clean['review_score'] >= 6.8, 'review_score_new'] = 0\n","    df_clean['review_score_new'] = df_clean['review_score_new'].astype('int32')\n","\n","    return df_clean"]},{"cell_type":"markdown","id":"FVTwGtjv3F3U","metadata":{"id":"FVTwGtjv3F3U"},"source":["### Create fit_transform function"]},{"cell_type":"code","execution_count":null,"id":"3BdB6S5_kI2i","metadata":{"id":"3BdB6S5_kI2i"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class Pre_Processing(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print('Khởi tạo đối tượng Pre-Processing')\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, data, y=None):\n","        import pickle\n","\n","        # load encoder model\n","        cv = pickle.load(open('model/CountVectorizer_self_model.sav','rb'))\n","\n","        # data cleaning\n","        df = data.copy()\n","        df_clean = data_cleaning(df)\n","\n","        # apply CountVectorizer\n","        # cv_transformed = cv.transform(df_clean['review_text_clean'].dropna())\n","        cv_transformed = cv.transform(df_clean['review_text_clean'])\n","        cv_transformed = cv_transformed.toarray()\n","        df_clean_cv = pd.DataFrame(cv_transformed, columns=cv.get_feature_names())\n","        df_clean_cv = pd.concat([df_clean.reset_index(), df_clean_cv], axis=1)\n","      \n","        return df_clean_cv"]},{"cell_type":"code","execution_count":null,"id":"8LUtfE7BkL8-","metadata":{"id":"8LUtfE7BkL8-"},"outputs":[],"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","class Create_XY(BaseEstimator, TransformerMixin):\n","    def __init__(self):\n","        print('Khởi tạo đối tượng Create_XY')\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, data, y=None):\n","        import pickle\n","        \n","        df_clean_cv = data\n","\n","        # create X, y   \n","        X = df_clean_cv.iloc[:,6:]\n","        y = df_clean_cv['review_score_new']\n","\n","        return X, y"]},{"cell_type":"code","execution_count":null,"id":"IT3UZOYz8sZy","metadata":{"id":"IT3UZOYz8sZy"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"function_lib.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.7.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"vscode":{"interpreter":{"hash":"5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"}}},"nbformat":4,"nbformat_minor":5}
